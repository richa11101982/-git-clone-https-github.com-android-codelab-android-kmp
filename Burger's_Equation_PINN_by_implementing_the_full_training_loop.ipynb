{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/richa11101982/-git-clone-https-github.com-android-codelab-android-kmp/blob/main/Burger's_Equation_PINN_by_implementing_the_full_training_loop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Excellent\\! Let's complete the development of your **Burger's Equation PINN** by implementing the full **training loop** and defining the optimization strategy in PyTorch.\n",
        "\n",
        "## üèãÔ∏è Step 5: The Training Loop and Optimization\n",
        "\n",
        "The training loop is where the three loss terms ($\\mathcal{L}_{IC}$, $\\mathcal{L}_{BC}$, $\\mathcal{L}_{Physics}$) are combined, gradients are computed, and the model's weights are updated.\n",
        "\n",
        "### 1\\. Initialization and Setup\n",
        "\n",
        "First, initialize the model, the optimizer, and prepare the data tensors."
      ],
      "metadata": {
        "id": "Z_qW0p3zR61S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model and Data Setup (continuing from previous steps)\n",
        "layers = [2, 20, 20, 20, 20, 1] # [x, t] -> 4 hidden layers -> [u]\n",
        "pinn_model = PINN(layers)\n",
        "optimizer = torch.optim.Adam(pinn_model.parameters(), lr=1e-3)\n",
        "criterion = nn.MSELoss() # PyTorch's Mean Squared Error\n",
        "\n",
        "# Get data points (assuming the create_data_tensors function is defined)\n",
        "x_ic, t_ic, u_ic_true, x_bc, t_bc, u_bc_true, x_physics, t_physics = create_data_tensors()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "qpJNZq0_R61U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### 2\\. The Training Function\n",
        "\n",
        "We'll define a function that performs one optimization step."
      ],
      "metadata": {
        "id": "3ctvoHKIR61V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(optimizer, model, criterion, nu, x_ic, t_ic, u_ic_true, x_bc, t_bc, u_bc_true, x_physics, t_physics):\n",
        "    optimizer.zero_grad() # Reset gradients\n",
        "\n",
        "    # --- 1. Compute Loss Terms ---\n",
        "\n",
        "    # A. Initial Condition (IC) Loss\n",
        "    u_ic_pred = model(x_ic, t_ic)\n",
        "    loss_ic = criterion(u_ic_true, u_ic_pred)\n",
        "\n",
        "    # B. Boundary Condition (BC) Loss\n",
        "    u_bc_pred = model(x_bc, t_bc)\n",
        "    loss_bc = criterion(u_bc_true, u_bc_pred)\n",
        "\n",
        "    # C. Physics Loss (The most critical part)\n",
        "    # The output f is the residual of the PDE (should be close to zero)\n",
        "    f_pred = physics_residual(model, x_physics, t_physics, nu)\n",
        "    # Target for physics residual is zero\n",
        "    loss_physics = criterion(f_pred, torch.zeros_like(f_pred))\n",
        "\n",
        "    # --- 2. Combine and Optimize ---\n",
        "\n",
        "    # The weights (lambda) balance the different loss terms.\n",
        "    # In practice, balancing these weights (e.g., using Adaptive Weights)\n",
        "    # is often the hardest part of PINN training. For now, we set them all to 1.0.\n",
        "    lambda_ic = 1.0\n",
        "    lambda_bc = 1.0\n",
        "    lambda_phys = 1.0\n",
        "\n",
        "    total_loss = lambda_ic * loss_ic + lambda_bc * loss_bc + lambda_phys * loss_physics\n",
        "\n",
        "    # Backpropagation\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return total_loss.item(), loss_ic.item(), loss_bc.item(), loss_physics.item()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "VRkL87u_R61W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### 3\\. Execution\n",
        "\n",
        "Finally, run the training for a number of epochs."
      ],
      "metadata": {
        "id": "ST09bLnAR61W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20000\n",
        "log_frequency = 1000\n",
        "\n",
        "print(f\"Starting PINN training for {epochs} epochs...\")\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    total_l, ic_l, bc_l, phys_l = train_step(\n",
        "        optimizer, pinn_model, criterion, nu,\n",
        "        x_ic, t_ic, u_ic_true, x_bc, t_bc, u_bc_true, x_physics, t_physics\n",
        "    )\n",
        "\n",
        "    if epoch % log_frequency == 0:\n",
        "        print(f\"--- Epoch {epoch}/{epochs} ---\")\n",
        "        print(f\"Total Loss: {total_l:.6e}\")\n",
        "        print(f\"  IC Loss: {ic_l:.6e}\")\n",
        "        print(f\"  BC Loss: {bc_l:.6e}\")\n",
        "        print(f\"  Physics Loss: {phys_l:.6e}\")\n",
        "\n",
        "print(\"Training finished.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "W6cd6rr5R61X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚ö†Ô∏è Key Challenges and Next Steps\n",
        "\n",
        "1.  **Loss Balancing ($\\lambda$):** For complex non-linear problems like Burger's equation, setting all $\\lambda$ weights to $1.0$ often fails. You may need to introduce techniques like **learning rate annealing** or **Adaptive Loss Weighting** to ensure the $\\mathcal{L}_{Physics}$ term doesn't dominate or become negligibly small early in training.\n",
        "2.  **Architecture:** The choice of hidden layers, neurons, and activation function (`Tanh` is good for PINNs) is crucial.\n",
        "3.  **Optimization:** After the initial Adam optimization, it is common to switch to a high-precision optimizer like **L-BFGS** for the final steps to achieve lower loss values.\n",
        "\n",
        "You have now built the complete structure of a non-linear PIML model. Your next step would be to execute this code and **visualize the results** to see how well the predicted solution $u_{NN}(x, t)$ matches the true solution (if one is available) or a known numerical solution.\n",
        "\n",
        "Would you like to explore methods for **visualizing and validating** the PINN solution?"
      ],
      "metadata": {
        "id": "c_sUwjkCR61X"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}