{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/richa11101982/-git-clone-https-github.com-android-codelab-android-kmp/blob/main/Burger's_Equation_Physics_Informed_Neural_Network_(PINN)_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great\\! Let's proceed with setting up the **PyTorch** structure for your **Burger's Equation Physics-Informed Neural Network (PINN)**.\n",
        "\n",
        "PyTorch is excellent for PINNs because its **dynamic computation graph** makes calculating the required second-order derivatives via automatic differentiation very straightforward.\n",
        "\n",
        "-----\n",
        "\n",
        "## ðŸ’» Step 1: Setting up the Environment\n",
        "\n",
        "You'll need PyTorch and NumPy."
      ],
      "metadata": {
        "id": "IInVtR4jQLKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import grad"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "id": "Ta-lHqJfQLKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "## ðŸ§  Step 2: Define the Neural Network ($u_{NN}$)\n",
        "\n",
        "This network takes $(x, t)$ as input and outputs $u$."
      ],
      "metadata": {
        "id": "OfyX-QGaQLKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PINN(nn.Module):\n",
        "    def __init__(self, layers):\n",
        "        super(PINN, self).__init__()\n",
        "        # Sequential model definition\n",
        "        self.model = nn.Sequential()\n",
        "\n",
        "        # Build the layers\n",
        "        for i in range(len(layers) - 2):\n",
        "            self.model.add_module(f\"layer_{i}\", nn.Linear(layers[i], layers[i+1]))\n",
        "            self.model.add_module(f\"activation_{i}\", nn.Tanh())\n",
        "\n",
        "        # Output layer (no activation for the final output u)\n",
        "        self.model.add_module(\"output\", nn.Linear(layers[-2], layers[-1]))\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        # Concatenate x and t as the input vector\n",
        "        return self.model(torch.cat([x, t], dim=1))\n",
        "\n",
        "# Example Configuration: 2 inputs (x, t), 4 hidden layers of 20 neurons, 1 output (u)\n",
        "# layers = [2, 20, 20, 20, 20, 1]\n",
        "# model = PINN(layers)"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "id": "OwHkIcsyQLKF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "## ðŸ§ª Step 3: Define the Physics Residual ($f$)\n",
        "\n",
        "This is the core function where automatic differentiation is performed to calculate the derivatives needed for the Burger's equation residual:\n",
        "\n",
        "$$f(x, t) = \\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} - \\nu \\frac{\\partial^2 u}{\\partial x^2}$$"
      ],
      "metadata": {
        "id": "ThOZ_ZH0QLKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def physics_residual(model, x_physics, t_physics, nu):\n",
        "    # Ensure inputs require gradient tracking\n",
        "    x = x_physics.clone().requires_grad_(True)\n",
        "    t = t_physics.clone().requires_grad_(True)\n",
        "\n",
        "    # 1. Network Output (u)\n",
        "    u = model(x, t)\n",
        "\n",
        "    # --- 2. Calculate First Derivatives (u_t and u_x) ---\n",
        "    # Create a tensor of ones for the vector-Jacobian product (vjp)\n",
        "    # The output 'u' is a column vector of solutions for all points\n",
        "    u_t_x = grad(u, (t, x), grad_outputs=torch.ones_like(u), retain_graph=True, create_graph=True)\n",
        "    u_t = u_t_x[0] # du/dt\n",
        "    u_x = u_t_x[1] # du/dx\n",
        "\n",
        "    # --- 3. Calculate Second Derivative (u_xx) ---\n",
        "    # The second derivative is found by taking the gradient of u_x with respect to x\n",
        "    u_xx = grad(u_x, x, grad_outputs=torch.ones_like(u_x), retain_graph=True, create_graph=True)[0]\n",
        "\n",
        "    # --- 4. Calculate the Residual (f) ---\n",
        "    # f = u_t + u * u_x - nu * u_xx\n",
        "    f = u_t + u * u_x - nu * u_xx\n",
        "\n",
        "    # We want f to be zero, so the loss is the MSE of f\n",
        "    return f"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "id": "7GvYPGPUQLKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "## ðŸ“Š Step 4: Data Sampling\n",
        "\n",
        "We need three sets of points for the three loss terms ($\\mathcal{L}_{IC}$, $\\mathcal{L}_{BC}$, $\\mathcal{L}_{Physics}$)."
      ],
      "metadata": {
        "id": "WqjYydZhQLKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "nu = 0.01 / np.pi # Viscosity term\n",
        "N_IC = 50        # Initial Condition points\n",
        "N_BC = 50        # Boundary Condition points\n",
        "N_PHYSICS = 1000 # Collocation/Physics points (most critical)\n",
        "\n",
        "# Convert all data to PyTorch Tensors\n",
        "def create_data_tensors():\n",
        "    # 1. Initial Condition (IC) Points: t=0, x in [-1, 1]\n",
        "    x_ic = torch.linspace(-1, 1, N_IC).view(-1, 1)\n",
        "    t_ic = torch.zeros_like(x_ic)\n",
        "    # True value for IC: u(x, 0) = -sin(pi * x)\n",
        "    u_ic_true = -torch.sin(np.pi * x_ic)\n",
        "\n",
        "    # 2. Boundary Condition (BC) Points: x=-1 and x=1, t in [0, 1]\n",
        "    t_bc = torch.rand(N_BC).view(-1, 1)\n",
        "    x_bc_neg1 = -torch.ones_like(t_bc) # x = -1\n",
        "    x_bc_pos1 = torch.ones_like(t_bc)  # x = 1\n",
        "    # True value for BC: u(-1, t) = 0 and u(1, t) = 0\n",
        "    u_bc_true = torch.zeros_like(t_bc)\n",
        "\n",
        "    # Concatenate boundary points\n",
        "    x_bc = torch.cat([x_bc_neg1, x_bc_pos1], dim=0)\n",
        "    t_bc = torch.cat([t_bc, t_bc], dim=0)\n",
        "\n",
        "\n",
        "    # 3. Physics (Collocation) Points: t in [0, 1], x in [-1, 1]\n",
        "    # Sample randomly in the entire spatio-temporal domain\n",
        "    x_physics = 2 * torch.rand(N_PHYSICS).view(-1, 1) - 1 # x in [-1, 1]\n",
        "    t_physics = torch.rand(N_PHYSICS).view(-1, 1)         # t in [0, 1]\n",
        "\n",
        "    return x_ic, t_ic, u_ic_true, x_bc, t_bc, u_bc_true, x_physics, t_physics\n",
        "\n",
        "# --- The full training loop would now assemble these components ---"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "id": "Pq_K_he2QLKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next logical step is to combine these components into the **training loop**, where the losses are calculated and the optimizer updates the network weights.\n",
        "\n",
        "Would you like to proceed with the **training loop setup** and optimization strategy?"
      ],
      "metadata": {
        "id": "IDUkZxmnQLKG"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}